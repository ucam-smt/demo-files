
\section rescoring Rescoring Procedures

\subsection rescoring_lm Efficient Language Model Rescoring

As discussed above, HiFST uses a lexicographic semiring (\ref basic_scores, [\ref Roark2011])
of two tropical weights.  In each arc of a lattice generated by HiFST,
the first weight (G+M) contains the correct score (translation grammar score + language model score).  The second weight G only contains the
translation grammar score.  An example is repeated here:

     > zcat output/exp.baseline/LATS/1.fst.gz | fstprint | head -n 10
     0 1    1		                 1             -2.609375,-2.609375
     1 36   999999999		 	 999999999	29.5185547,29.5185547
     1 35   12198			 12198		19.6816635,7.27929688
     1 34   12198			 12198		17.9902573,5.58789062
     1 33   9227			 9227		16.845089,2.56347656
     1 32   9121			 9121		9.55193996,-1.04199219
     1 31   9121			 9121		7.86053371,-2.73339844
     1 30   9121			 9121		9.33318996,-1.26074219
     1 29   8559			 8559		14.9520674,4.25878906
     1 28   4608			 4608		17.6058693,4.88671875

The advantage of using the lexicographic semiring to represent (G+M,G)
weights is that the language model score can be substracted very
efficiently.  HiFST binaries do this mapping internally with an
[fstmap](http://www.openfst.org/twiki/bin/view/FST/ArcMapDoc)
operation.  The result is a WFSA whose weights contain only the translation
grammar scores (G,G).  The lexmap tool can be used to do this mapping, as follows, yielding lexicographic weights (G,G):

      > zcat output/exp.baseline.outputnoprune/LATS/1.fst.gz | lexmap.O2  | fstprint | head -n 10
      0	 1 1	1						-2.609375,-2.609375
      1	 36	999999999					999999999	29.5185547,29.5185547
      1	 35	12198						12198		7.27929688,7.27929688
      1	 34	12198						12198		5.58789062,5.58789062
      1	 33	9227						9227		2.56347656,2.56347656
      1	 32	9121						9121		-1.04199219,-1.04199219
      1	 31	9121						9121		-2.73339844,-2.73339844
      1	 30	9121						9121		-1.26074219,-1.26074219
      1	 29	8559						8559		4.25878906,4.25878906
      1	 28	4608						4608		4.88671875,4.88671875

Using this  facility to remove language model scores,
the HiFST applylm tool can be used to rescore lattices under a different language model than was used in first-pass translation.  Operations are as follows:

   -# A lattice with lexicographic (G+M,G) weights is loaded
   -# Weights are converted to (G,G) via fstmap
   -# The new language model(s) are applied via composition under the lexicographic semiring, with optional scale factors and word insertion penalties.  The new WFSAs weights are of the form (G+M',G), where M' are the new language model weights.
   -# The reweighted WFSA is written to disk, with either lexicographic or standard tropical weights

The following example uses applylm to rescore lattices generated with
almost no pruning (`output/exp.baseline.outputnoprune/LATS`). Rescoring
uses the same 4-gram language model originally used to generate the
lattice, but with a different scale factor (`lm.scale=0.9`).

     > applylm.O2 --config=configs/CF.baseline.outputnoprune.lmrescore  &> log/log.lmrescore

For the first sentence, the original 1-best hypothesis was:

    > zcat output/exp.baseline.outputnoprune/LATS/1.fst.gz | printstrings.O2 --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2>/dev/null
    <s> republican strategy of resistance to the renewal of obamas election </s>	57.4707,-8.03809

Rescoring yields a slightly different 1-best:

    > zcat output/exp.baseline.lmrescore/LATS/1.fst.gz | printstrings.O2 --semiring=lexstdarc -m wmaps/wmt13.en.wmap -w 2>/dev/null
    <s> the republican strategy of resistance to the renewal of obamas election </s>	50.9163,-8.66992

Note the "load.deletelmcost" option in the configuration file, which
instructs the tool to subtract old lm scores first.

If the scaling is not changed, both lattices should be identical
(`applylm.O2 --config=configs/CF.baseline.outputnoprune.lmrescore --lm.featureweights=1`).







